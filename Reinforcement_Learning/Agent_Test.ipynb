{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNNtbNQ4PSCrYKZS+D+1Dq2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CYjqYjtnGiY6","executionInfo":{"status":"ok","timestamp":1710171197549,"user_tz":300,"elapsed":172,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}},"outputId":"aa91d0c3-d8ee-4349-ceda-5efb98899cda"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 2\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 3\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n","Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 4\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 5\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 6\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 7\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n","Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n","Estado actual: 1, Acción: avanzar, Nuevo estado: 2\n","El agente no ha alcanzado el objetivo.\n","Episode: 8\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n","Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 9\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n","Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n","Episode: 10\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: retroceder, Nuevo estado: 0\n","Estado actual: 0, Acción: avanzar, Nuevo estado: 1\n","Estado actual: 1, Acción: retroceder, Nuevo estado: 0\n","El agente no ha alcanzado el objetivo.\n"]}],"source":["import random\n","import numpy as np\n","\n","class Environment:\n","    def __init__(self):\n","        self.steps = 10\n","        self.board = [False] * 5\n","        self.board[4] = True  # El objetivo está en el último índice\n","\n","    def reset(self):\n","        self.steps = 10\n","        self.board = [False] * 5\n","        self.board[4] = True\n","\n","    def start(self):\n","        return 0\n","\n","    def end(self):\n","        return len(self.board)\n","\n","    def is_done(self):\n","        return self.steps == 0\n","\n","    def action(self, state):\n","        if state < 0 or state >= len(self.board):\n","            return False\n","        self.steps -= 1\n","        return self.board[state]\n","\n","class Agent:\n","    def __init__(self):\n","        self.state = 0\n","        self.actions = [0, 1]  # 0 para retroceder, 1 para avanzar\n","\n","    def reset(self):\n","        self.state = 0\n","\n","    def perform_action(self, action, env):\n","        if action == 1:\n","            self.state = min(self.state + 1, env.end() - 1)\n","        else:\n","            self.state = max(self.state - 1, env.start())\n","        return env.action(self.state)\n","\n","    def get_action_name(self, action):\n","        return \"avanzar\" if action == 1 else \"retroceder\"\n","\n","class Learner:\n","    def __init__(self, agent, env, alpha=0.1, gamma=0.6, epsilon=0.1):\n","        self.agent = agent\n","        self.env = env\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.table = np.zeros((env.end(), 2))\n","\n","    def run(self):\n","        self.env.reset()\n","        self.agent.reset()\n","        done = False\n","        while not done:\n","            current_state = self.agent.state\n","            if random.uniform(0, 1) < self.epsilon:\n","                action = random.choice(self.agent.actions)\n","            else:\n","                action = np.argmax(self.table[current_state])\n","            reward = self.agent.perform_action(action, self.env)\n","            print(f\"Estado actual: {current_state}, Acción: {self.agent.get_action_name(action)}, Nuevo estado: {self.agent.state}\")\n","            done = self.env.is_done()\n","            next_state = self.agent.state\n","            old_value = self.table[current_state, action]\n","            next_max = np.max(self.table[next_state])\n","            new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n","            self.table[current_state, action] = new_value\n","        if self.env.board[self.agent.state]:\n","            print(\"El agente ha alcanzado el objetivo.\")\n","        else:\n","            print(\"El agente no ha alcanzado el objetivo.\")\n","\n","def main():\n","    episodes = 10\n","    e = Environment()\n","    a = Agent()\n","    learner = Learner(a, e)\n","    for i in range(episodes):\n","        print(f\"Episode: {i+1}\")\n","        learner.run()\n","\n","main()"]}]}