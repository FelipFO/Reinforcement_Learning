{"cells":[{"cell_type":"code","execution_count":1,"id":"352e05fd-bdce-4361-8510-31d062d3f8de","metadata":{"id":"352e05fd-bdce-4361-8510-31d062d3f8de","executionInfo":{"status":"ok","timestamp":1710169887116,"user_tz":300,"elapsed":639,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"outputs":[],"source":["class Environment:\n","    def __init__(self):\n","        self.steps = 10\n","        self.board = [False for x in range(0, 5)]\n","        self.board[4] = True\n","\n","    def reset(self):\n","        self.steps = 10\n","\n","    def start(self):\n","        return 0\n","\n","    def end(self):\n","        return len(self.board)\n","\n","    def is_done(self) -> bool:\n","        return self.steps == 0\n","\n","    def action(self, state) -> bool:\n","        self.steps -= 1\n","        if self.is_done():\n","            print(\"Did not reach the goal\")\n","        return self.board[state]\n",""]},{"cell_type":"code","execution_count":4,"id":"b6ca87dd-4171-4fb0-9e9b-6b5b393d958b","metadata":{"id":"b6ca87dd-4171-4fb0-9e9b-6b5b393d958b","executionInfo":{"status":"ok","timestamp":1710169917092,"user_tz":300,"elapsed":240,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"outputs":[],"source":["import random\n","\n","class Agent:\n","    def __init__(self):\n","        self.state = 0\n","        self.actions = [0,1]\n","\n","    def reset(self):\n","        self.state = 0\n","\n","    def forward(self, limit):\n","        self.state = min(self.state + 1, limit-1)\n","\n","    def back(self, lowLimit):\n","        self.state = max(self.state - 1, lowLimit)\n","\n","    def action(self, env: Environment):\n","        action = random.choice(self.actions)\n","        if action:\n","            self.forward(env.end())\n","        else:\n","            self.back(env.start())\n","        return env.action(self.state)\n",""]},{"cell_type":"code","execution_count":5,"id":"21efb772-8d63-4e22-9ca1-b1967a2889fd","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"21efb772-8d63-4e22-9ca1-b1967a2889fd","executionInfo":{"status":"ok","timestamp":1710169919556,"user_tz":300,"elapsed":239,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}},"outputId":"e73e64ed-76a8-48cb-e897-a04b489ab460"},"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 1\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 2\n","Did not reach the goal\n","1 success - 9 Fail\n","Episode: 3\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 4\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 5\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 6\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 7\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 8\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 9\n","Did not reach the goal\n","0 success - 10 Fail\n","Episode: 10\n","Did not reach the goal\n","0 success - 10 Fail\n"]}],"source":["def main():\n","    episodes = 10\n","    a = Agent()\n","    e = Environment()\n","    for i in range(0, episodes):\n","        print(f\"Episode: {i+1}\")\n","        count = 0\n","        while not e.is_done():\n","            count += a.action(e)\n","        print(f\"{count} success - {episodes - count} Fail\")\n","        a.reset()\n","        e.reset()\n","\n","main()"]},{"cell_type":"code","source":["#Paso 1. Definir la clase Learner\n","import random\n","\n","class Learner:\n","    def __init__(self, agent, env, alpha=0.1, gamma=0.6, epsilon=0.1):\n","        self.agent = agent\n","        self.env = env\n","        self.alpha = alpha\n","        self.gamma = gamma\n","        self.epsilon = epsilon\n","        self.table = self.__inittable__()\n","\n","    def __inittable__(self):\n","        # Inicializar la tabla con ceros. La dimensión es el número de estados x número de acciones.\n","        # Como hay 5 posiciones (estados) y 2 acciones (avanzar, retroceder), la tabla será de 5x2.\n","        return np.zeros((self.env.end(), len(self.agent.actions)))\n","\n","\n","#Paso 2. Incluyendo la ejecucion:\n","    def run(self):\n","        done = False\n","        while not done:\n","            current_state = self.agent.state\n","            # Decidir entre tomar una acción al azar o la mejor acción basada en epsilon\n","            if random.uniform(0, 1) < self.epsilon:\n","\n","     #Paso 3. Escogencia de acción aleatoria:\n","\n","                action = random.choice(self.agent.actions)\n","            else:\n","                # Mejor acción basada en la tabla de aprendizaje\n","                action = np.argmax(self.table[current_state])\n","\n","            # Realizar la acción y obtener el nuevo estado, recompensa, y si se terminó\n","            reward = self.env.action(current_state)\n","            done = self.env.is_done()\n","            next_state = self.agent.state\n","\n","            # Calcular el valor actual y el siguiente mejor valor\n","            old_value = self.table[current_state, action]\n","            next_max = np.max(self.table[next_state])\n","\n","            # Calcular el nuevo valor\n","            new_value = (1 - self.alpha) * old_value + self.alpha * (reward + self.gamma * next_max)\n","\n","            # Actualizar la tabla\n","            self.table[current_state, action] = new_value\n","\n","            if not done:\n","                # Actualizar el estado del agente si no se ha terminado\n","                if action == 1:\n","                    self.agent.forward(self.env.end())\n","                else:\n","                    self.agent.back(self.env.start())\n","\n","    def step(self, action):\n","          # Calcular el estado actual del agente\n","          old_state = self.agent.state\n","\n","          # Obtener la recompensa para el estado actual y la acción tomada\n","          reward = self.getReward(old_state, action)\n","\n","          # Avanzar el estado del agente ejecutando la acción tomada\n","          if action == 0:\n","              self.agent.back(self.env.start())\n","          else:\n","              self.agent.forward(self.env.end())\n","\n","          # Calcula el nuevo estado del agente\n","          new_state = self.agent.state\n","\n","          # Retorna el viejo estado, la recompensa, y el nuevo estado\n","          return old_state, reward, new_state\n","\n","    def getReward(self, state, action):\n","        # Define la recompensa base como 0\n","        reward = 0\n","        # Define si el episodio ha terminado\n","        done = False\n","        if state == 4:\n","            reward = 10\n","            done = True\n","        else:\n","            pass\n","\n","        return reward, done\n"],"metadata":{"id":"56QX4wk0Bcn-","executionInfo":{"status":"ok","timestamp":1710170068941,"user_tz":300,"elapsed":197,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"id":"56QX4wk0Bcn-","execution_count":7,"outputs":[]},{"cell_type":"code","source":["class Learner:\n","    # Asumir que el resto de la clase Learner ya está definido anteriormente\n","\n","    def step(self, action):\n","        # Calcula el estado actual del agente\n","        old_state = self.agent.state\n","\n","        # Obtener la recompensa para el estado actual y la acción tomada\n","        reward = self.getReward(old_state, action)\n","\n","        # Avanza el estado del agente ejecutando la acción tomada\n","        # Esto puede variar según cómo esté implementado tu agente.\n","        # Por ejemplo, si tu agente tiene métodos forward y back:\n","        if action == 0:  # Suponiendo que 0 representa retroceder y 1 avanzar\n","            self.agent.back(self.env.start())\n","        else:\n","            self.agent.forward(self.env.end())\n","\n","        # Calcula el nuevo estado del agente\n","        new_state = self.agent.state\n","\n","        # Retorna el viejo estado, la recompensa, y el nuevo estado\n","        return old_state, reward, new_state\n","\n","    def getReward(self, state, action):\n","        # Esta función necesita ser implementada para calcular la recompensa\n","        # basada en el estado y la acción. Esto es solo un placeholder.\n","        # Necesitarás implementar la lógica específica para obtener la recompensa.\n","        return 0  # Placeholder para la recompensa\n"],"metadata":{"id":"d6RqqZDBFQba","executionInfo":{"status":"ok","timestamp":1710170166838,"user_tz":300,"elapsed":263,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"id":"d6RqqZDBFQba","execution_count":8,"outputs":[]},{"cell_type":"code","source":["class Learner:\n","    # Asumir que el resto de la clase Learner ya está definido anteriormente\n","\n","    def getReward(self, state, action):\n","        # Define la recompensa base como 0\n","        reward = 0\n","        # Define si el episodio ha terminado\n","        done = False\n","\n","        # Si el agente alcanza la posición más a la derecha del tablero (estado 4),\n","        # se le recompensa con 10 y se termina el episodio\n","        if state == 4:\n","            reward = 10\n","            done = True\n","        else:\n","            # Si no, puedes definir otras recompensas para diferentes estados/actions aquí\n","            # Por ejemplo, penalizaciones o recompensas menores por aproximarse al objetivo\n","            pass\n","\n","        return reward, done\n"],"metadata":{"id":"9IQJRohiFSXC","executionInfo":{"status":"ok","timestamp":1710170169818,"user_tz":300,"elapsed":222,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"id":"9IQJRohiFSXC","execution_count":9,"outputs":[]},{"cell_type":"code","source":["class Environment:\n","    def __init__(self):\n","        self.board = [False for x in range(0, 5)]\n","        self.board[4] = True\n","\n","    def start(self):\n","        return 0\n","\n","    def end(self):\n","        return len(self.board)\n"],"metadata":{"id":"It_rmFPAFWF6","executionInfo":{"status":"ok","timestamp":1710170172100,"user_tz":300,"elapsed":191,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"id":"It_rmFPAFWF6","execution_count":10,"outputs":[]},{"cell_type":"code","source":["class Agent:\n","    def __init__(self):\n","        self.state = 0\n","        self.actions = [0, 1]  # 0 para \"back\", 1 para \"forward\"\n","        self.rightBound = rightBound  # Tamaño del tablero\n","\n","    def reset(self):\n","        self.state = 0\n","\n","    def forward(self):\n","        # Modificado para no recibir parámetros; usa rightBound para limitar el estado\n","        self.state = min(self.state + 1, self.rightBound - 1)\n","\n","    def back(self):\n","        # Modificado para no recibir parámetros\n","        self.state = max(self.state - 1, 0)\n","\n","    def action(self, action_id):\n","        # Modificado para recibir un identificador de acción\n","        if action_id == 1:\n","            self.forward()\n","        else:\n","            self.back()\n","\n","    def getAction(self, action_number):\n","        # Retorna el nombre de la acción basado en el número de acción\n","        return \"forward\" if action_number == 1 else \"back\"\n"],"metadata":{"id":"mR_4DM4dFX6K","executionInfo":{"status":"ok","timestamp":1710170231339,"user_tz":300,"elapsed":255,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}}},"id":"mR_4DM4dFX6K","execution_count":14,"outputs":[]},{"cell_type":"code","source":["# Asegúrate de que las clases Agent, Environment y Learner estén definidas correctamente\n","# según las instrucciones anteriores\n","\n","def main():\n","    episodes = 10\n","    e = Environment()  # Asumiendo que el ambiente no necesita parámetros o están ya configurados\n","    a = Agent()  # Pasamos el tamaño del tablero como el límite derecho\n","    learner = Learner(agent=a, env=e)  # Crea el objeto Learner con el agente y el ambiente\n","\n","    for i in range(episodes):\n","        print(f\"Episode: {i+1}\")\n","        learner.run()  # Ejecuta el loop de aprendizaje del agente mediante Learner\n","        a.reset()  # Solo reseteamos el estado del agente para el próximo episodio\n","\n","main()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"0pynRQb0FZ_q","executionInfo":{"status":"error","timestamp":1710170233019,"user_tz":300,"elapsed":201,"user":{"displayName":"Andrés Felipe Flórez Olivera","userId":"07987645950168172278"}},"outputId":"c7b2af5b-c679-494e-e63b-d15b330f855f"},"id":"0pynRQb0FZ_q","execution_count":15,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'rightBound' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-15-ad07c5a53cfc>\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Solo reseteamos el estado del agente para el próximo episodio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-15-ad07c5a53cfc>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mepisodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0me\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Asumiendo que el ambiente no necesita parámetros o están ya configurados\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Pasamos el tamaño del tablero como el límite derecho\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlearner\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Crea el objeto Learner con el agente y el ambiente\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-14-611631293678>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# 0 para \"back\", 1 para \"forward\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrightBound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrightBound\u001b[0m  \u001b[0;31m# Tamaño del tablero\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'rightBound' is not defined"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3.8.9 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.9"},"vscode":{"interpreter":{"hash":"31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"}},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}